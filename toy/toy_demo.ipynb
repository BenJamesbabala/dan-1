{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Demo: Distributional Adversarial Networks on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this notebook, we test the adversarial network methods on a simple a toy dataset consisting of a 2D mixture of gaussians, where traditional single-sample-point methods such as GAN are known to struggle with mode collapse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model      = 'dan_s' # One of: [gan,dan_s,dan_2s]\n",
    "batch_size = 512\n",
    "Z_dim      = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generator and Discriminator Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n",
    "    G_h3 = tf.nn.relu(tf.matmul(G_h2, G_W3) + G_b3)\n",
    "    G_logit = tf.matmul(G_h3, G_W4) + G_b4\n",
    "    return G_logit\n",
    "\n",
    "def gan_discriminator(x_real, x_fake):\n",
    "    \"\"\" I.e. Regular GAN discriminator \"\"\"\n",
    "    D_h1_real = tf.nn.relu(tf.matmul(x_real / 4, D_W1) + D_b1)\n",
    "    D_h2_real = tf.nn.relu(tf.matmul(D_h1_real, D_W2) + D_b2)\n",
    "    D_h3_real = tf.nn.relu(tf.matmul(D_h2_real, D_W3) + D_b3)\n",
    "    D_logit_real = tf.matmul(D_h3_real, D_W4) + D_b4\n",
    "    D_prob_real = tf.nn.sigmoid(D_logit_real)\n",
    "\n",
    "    D_h1_fake = tf.nn.relu(tf.matmul(x_fake / 4, D_W1) + D_b1)\n",
    "    D_h2_fake = tf.nn.relu(tf.matmul(D_h1_fake, D_W2) + D_b2)\n",
    "    D_h3_fake = tf.nn.relu(tf.matmul(D_h2_fake, D_W3) + D_b3)\n",
    "    D_logit_fake = tf.matmul(D_h3_fake, D_W4) + D_b4\n",
    "    D_prob_fake = tf.nn.sigmoid(D_logit_fake)    \n",
    "    return D_prob_real, D_logit_real, D_prob_fake, D_logit_fake\n",
    "\n",
    "def dan_s_discriminator(x_real, x_fake):\n",
    "    \"\"\" A.k.a Sample classifier for DAN_S\"\"\"\n",
    "    D_h1_real = tf.nn.relu(tf.matmul(x_real / 4, D_W1) + D_b1)\n",
    "    D_h2_real = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_real, D_W2) + D_b2), axis=0, keep_dims=True)\n",
    "    D_h3_real = tf.nn.relu(tf.matmul(D_h2_real, D_W3) + D_b3)\n",
    "    D_logit_real = tf.matmul(D_h3_real, D_W4) + D_b4\n",
    "    D_prob_real = tf.nn.sigmoid(D_logit_real)\n",
    "\n",
    "    D_h1_fake = tf.nn.relu(tf.matmul(x_fake / 4, D_W1) + D_b1)\n",
    "    D_h2_fake = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_fake, D_W2) + D_b2), axis=0, keep_dims=True)\n",
    "    D_h3_fake = tf.nn.relu(tf.matmul(D_h2_fake, D_W3) + D_b3)\n",
    "    D_logit_fake = tf.matmul(D_h3_fake, D_W4) + D_b4\n",
    "    D_prob_fake = tf.nn.sigmoid(D_logit_fake)\n",
    "\n",
    "    return D_prob_real, D_logit_real, D_prob_fake, D_logit_fake\n",
    "                        \n",
    "def dan_2s_discriminator(x_ori_1, x_ori_2, x_alter_1, x_alter_2):\n",
    "    \"\"\" A.k.a two-sample discriminator\"\"\"\n",
    "    D_h1_ori_1 = tf.nn.relu(tf.matmul(x_ori_1 / 4, D_W1) + D_b1)\n",
    "    D_h2_ori_1 = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_ori_1, D_W2) + D_b2), axis=0, keep_dims=True)\n",
    "    D_h1_alter_1 = tf.nn.relu(tf.matmul(x_alter_1 / 4, D_W1) + D_b1)\n",
    "    D_h2_alter_1 = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_alter_1, D_W2) + D_b2), axis=0, keep_dims=True)\n",
    "    D_h1_ori_2 = tf.nn.relu(tf.matmul(x_ori_2 / 4, D_W1) + D_b1)\n",
    "    D_h2_ori_2 = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_ori_2, D_W2) + D_b2), axis=0, keep_dims=True)\n",
    "    D_h1_alter_2 = tf.nn.relu(tf.matmul(x_alter_2 / 4, D_W1) + D_b1)\n",
    "    D_h2_alter_2 = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_alter_2, D_W2) + D_b2), axis=0, keep_dims=True)\n",
    "\n",
    "    D_h3_11 = tf.nn.relu(tf.matmul(tf.abs(D_h2_ori_2 - D_h2_ori_1), D_W3) + D_b3)\n",
    "    D_h3_10 = tf.nn.relu(tf.matmul(tf.abs(D_h2_ori_2 - D_h2_alter_1), D_W3) + D_b3)\n",
    "    D_h3_01 = tf.nn.relu(tf.matmul(tf.abs(D_h2_alter_2 - D_h2_ori_1), D_W3) + D_b3)\n",
    "    D_h3_00 = tf.nn.relu(tf.matmul(tf.abs(D_h2_alter_2 - D_h2_alter_1), D_W3) + D_b3)\n",
    "\n",
    "    D_logit_00 = tf.matmul(D_h3_00, D_W4) + D_b4\n",
    "    D_logit_01 = tf.matmul(D_h3_01, D_W4) + D_b4\n",
    "    D_logit_10 = tf.matmul(D_h3_10, D_W4) + D_b4\n",
    "    D_logit_11 = tf.matmul(D_h3_11, D_W4) + D_b4\n",
    "    return D_logit_00, D_logit_11, D_logit_01, D_logit_10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.normal(size=[m, n])\n",
    "\n",
    "def read():\n",
    "    fid = open('gaussian.txt', 'r')\n",
    "    lines = fid.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        data.append([float(curr_num) for curr_num in line.split()])\n",
    "    return np.array(data)\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set up weights\n",
    "D_W1 = tf.Variable(xavier_init([2, 32]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[32]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([32, 32]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[32]))\n",
    "\n",
    "D_W3 = tf.Variable(xavier_init([32, 32]))\n",
    "D_b3 = tf.Variable(tf.zeros(shape=[32]))\n",
    "\n",
    "D_W4 = tf.Variable(xavier_init([32, 1]))\n",
    "D_b4 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3, D_W4, D_b4]\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([Z_dim, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 128]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W3 = tf.Variable(xavier_init([128,128]))\n",
    "G_b3 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W4 = tf.Variable(xavier_init([128, 2]))\n",
    "G_b4 = tf.Variable(tf.zeros(shape=[2]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3, G_W4, G_b4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set up variables\n",
    "if model in ['gan', 'dan_s']:\n",
    "    # Only one sample in this case\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    Z = tf.placeholder(tf.float32, shape=[None, Z_dim])    \n",
    "    G_sample = generator(Z)    \n",
    "    if model == 'gan':\n",
    "        D_prob_real, D_logit_real, D_prob_fake, D_logit_fake = gan_discriminator(X, G_sample)    \n",
    "    else:\n",
    "        D_prob_real, D_logit_real, D_prob_fake, D_logit_fake = dan_s_discriminator(X, G_sample)    \n",
    "        \n",
    "else:\n",
    "    # One variable per sample\n",
    "    X_1 = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    X_2 = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    Z_1 = tf.placeholder(tf.float32, shape=[None, Z_dim])\n",
    "    Z_2 = tf.placeholder(tf.float32, shape=[None, Z_dim])\n",
    "    G_sample_1 = generator(Z_1)\n",
    "    G_sample_2 = generator(Z_2)    \n",
    "    D_logit_00, D_logit_11, D_logit_01, D_logit_10 = dan_2s_discriminator(X_1, X_2, G_sample_1, G_sample_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set up losses\n",
    "if model in ['gan', 'dan_s']:\n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "else:\n",
    "    D_loss_00 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_00, labels=tf.zeros_like(D_logit_00)))\n",
    "    D_loss_01 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_01, labels=tf.ones_like(D_logit_01)))\n",
    "    D_loss_10 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_10, labels=tf.ones_like(D_logit_10)))\n",
    "    D_loss_11 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_11, labels=tf.zeros_like(D_logit_11)))\n",
    "    G_loss_01 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_01, labels=tf.zeros_like(D_logit_01)))\n",
    "    G_loss_10 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_10, labels=tf.ones_like(D_logit_10)))\n",
    "\n",
    "    D_loss = D_loss_00 + D_loss_01 + D_loss_10 + D_loss_11\n",
    "    G_loss = G_loss_01 + G_loss_10\n",
    "\n",
    "# Set up solvers\n",
    "D_solver = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5).minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5).minimize(G_loss, var_list=theta_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'gaussian.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ef9d39fca2fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-130e413b2355>\u001b[0m in \u001b[0;36mread\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gaussian.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'gaussian.txt'"
     ]
    }
   ],
   "source": [
    "data = read()\n",
    "data_size = len(data)\n",
    "data_size = 40000\n",
    "data = np.concatenate((data, data[:batch_size,:]), axis=0)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "save_fig_path = 'out_' + model\n",
    "if not os.path.exists(save_fig_path):\n",
    "    os.makedirs(save_fig_path)\n",
    "\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plot ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(data[:1000,0], data[:1000,1], 'b.')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-4,4])\n",
    "axes.set_ylim([-4,4])\n",
    "plt.title('True data distribution')\n",
    "plt.savefig(save_fig_path + '/real.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np_samples = []\n",
    "plot_every = 2000\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "for it in range(30000):\n",
    "    start_idx = it*batch_size%data_size\n",
    "    X_mb = data[start_idx:start_idx+batch_size, :]\n",
    "\n",
    "    if model in ['gan', 'dan_s']:\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(batch_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={X: X_mb, Z: sample_Z(batch_size, Z_dim)})\n",
    "    else:\n",
    "        sample_size = int(batch_size/2)\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X_1: X_mb[:sample_size,:], \\\n",
    "                                    X_2: X_mb[sample_size:,:], \\\n",
    "                                    Z_1: sample_Z(sample_size, Z_dim), \\\n",
    "                                    Z_2: sample_Z(sample_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={ \\\n",
    "                                    X_1: X_mb[:sample_size,:], \\\n",
    "                                    X_2: X_mb[sample_size:,:], \\\n",
    "                                    Z_1: sample_Z(sample_size, Z_dim), \\\n",
    "                                    Z_2: sample_Z(sample_size, Z_dim)})            \n",
    "\n",
    "    if (it+1) % plot_every == 0:\n",
    "        if model in ['gan', 'dan_s']:\n",
    "            samples = sess.run(G_sample, feed_dict={Z: sample_Z(1000, Z_dim)})\n",
    "        else:\n",
    "            samples = sess.run(G_sample_1, feed_dict={Z_1: sample_Z(1000, Z_dim)})\n",
    "\n",
    "        np_samples.append(samples)\n",
    "        plt.clf()\n",
    "        plt.plot(samples[:,0], samples[:,1], 'b.')\n",
    "        axes = plt.gca()\n",
    "        axes.set_xlim([-4,4])\n",
    "        axes.set_ylim([-4,4])\n",
    "        plt.title('Iter: {}, loss(D): {:2.2f}, loss(G):{:2.2f}'.format(it+1, D_loss_curr, G_loss_curr))\n",
    "        plt.savefig('out_' + model + '/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')        \n",
    "        display.display(plt.gcf()) \n",
    "        display.clear_output(wait=True)\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "xmax = 3\n",
    "\n",
    "np_samples_ = np_samples[::1]\n",
    "cols = len(np_samples_)\n",
    "bg_color  = sns.color_palette('Oranges', n_colors=256)[0]\n",
    "plt.figure(figsize=(2*cols, 2))\n",
    "for i, samps in enumerate(np_samples_):\n",
    "    if i == 0:\n",
    "        ax = plt.subplot(1,cols,1)\n",
    "    else:\n",
    "        plt.subplot(1,cols,i+1, sharex=ax, sharey=ax)\n",
    "    ax2 = sns.kdeplot(samps[:, 0], samps[:, 1], shade=True, cmap='Greens', n_levels=20, clip=[[-xmax,xmax]]*2)\n",
    "    ax2.set_facecolor(bg_color)\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.title('step %d'%(i*plot_every))\n",
    "ax.set_ylabel('%s'% model)\n",
    "plt.gcf().tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
